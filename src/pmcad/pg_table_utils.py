import os
from io import StringIO
import json
import subprocess
from .core import pg_exec
import pandas as pd
from tqdm import tqdm
import tempfile
import time


def pg_get_table(dbpath, table_name, limit=None):
    if limit is not None:
        sql = f"COPY (SELECT * FROM {table_name} LIMIT {limit}) TO STDOUT WITH (FORMAT CSV, DELIMITER E'\t', HEADER);"
    else:
        sql = f"COPY (SELECT * FROM {table_name}) TO STDOUT WITH (FORMAT CSV, DELIMITER E'\t', HEADER);"
    output = pg_exec(dbpath, sql=sql)
    return pd.read_csv(StringIO(output), sep="\t")


def pg_get_by_index(
    dbpath, table_name, index_col, index_val, use_index=None, explain=False
):
    """
    æ ¹æ®ç´¢å¼•åˆ—å€¼æŸ¥è¯¢ PostgreSQL è¡¨ï¼Œå¯é€‰æŒ‡å®šç´¢å¼•ã€‚
    """
    if isinstance(index_val, str):
        condition = f"{index_col} = '{index_val}'"
    else:
        condition = f"{index_col} = {index_val}"

    sql_parts = []

    # å¯é€‰ï¼šæ§åˆ¶ planner è¡Œä¸º
    if use_index:
        sql_parts += [
            "SET enable_seqscan = off;",
            "SET enable_bitmapscan = off;",
            "SET enable_indexscan = on;",
        ]

    # å¯é€‰ï¼šä»…è°ƒè¯•æ—¶æŸ¥çœ‹æ‰§è¡Œè®¡åˆ’
    if explain:
        explain_sql = f"EXPLAIN ANALYZE SELECT * FROM {table_name} WHERE {condition};"
        plan = pg_exec(dbpath, sql=explain_sql)
        print("ğŸ” Query Plan:\n", plan)

    # çœŸæ­£çš„æ•°æ®å¯¼å‡º
    sql_parts.append(
        f"""
    COPY (
        SELECT * FROM {table_name}
        WHERE {condition}
    ) TO STDOUT WITH (FORMAT CSV, DELIMITER E'\\t', HEADER);
    """
    )

    sql = "\n".join(sql_parts)
    output = pg_exec(dbpath, sql=sql)

    # é˜²å¾¡æ€§ï¼šè¿‡æ»¤é CSV å†…å®¹ï¼ˆæœ‰æ—¶å‰å‡ è¡Œå¯èƒ½åŒ…å« noticeï¼‰
    csv_part = "\n".join(
        [line for line in output.splitlines() if "\t" in line or "accession" in line]
    )

    if not csv_part.strip():
        return pd.DataFrame()

    return pd.read_csv(StringIO(csv_part), sep="\t")


def convert_gene_to_protein(dbpath, df_gene_go, batch_size=5000):
    """
    å°† (gene_id, go_id, weight, ...) æ˜ å°„ä¸º (uniprot_id, gene_id, go_id, weight, ...)
    ä»…æŸ¥è¯¢ df_gene_go ä¸­æ¶‰åŠçš„ GeneIDï¼ˆè‡ªåŠ¨å»é‡ + åˆ†æ‰¹å¤„ç†ï¼‰
    ä¿ç•™åŸå§‹ df_gene_go çš„æ‰€æœ‰åˆ—ã€‚
    """
    # å»é‡åçš„ GeneID åˆ—è¡¨
    gene_ids = df_gene_go["gene_id"].astype(str).unique().tolist()
    print(f"ğŸ” æ£€æµ‹åˆ° {len(gene_ids)} ä¸ªå”¯ä¸€åŸºå› IDï¼Œå¼€å§‹åˆ†æ‰¹æŸ¥è¯¢...")

    all_batches = []
    for i in range(0, len(gene_ids), batch_size):
        batch = gene_ids[i : i + batch_size]
        id_list = "', '".join(batch)

        sql = f"""
        COPY (
            SELECT db_id AS gene_id, uniprot_id
            FROM uniprot_idmapping
            WHERE db_name = 'GeneID'
              AND db_id IN ('{id_list}')
        ) TO STDOUT WITH (FORMAT CSV, DELIMITER E'\\t', HEADER);
        """
        output = pg_exec(dbpath, sql=sql)
        if not output.strip():
            continue

        batch_df = pd.read_csv(StringIO(output), sep="\t")
        all_batches.append(batch_df)
        print(f"  âœ… å·²å®Œæˆ {i + len(batch)} / {len(gene_ids)}")

    # åˆå¹¶æ‰€æœ‰æ‰¹æ¬¡
    if not all_batches:
        print("âš ï¸ æœªæ‰¾åˆ°ä»»ä½•æ˜ å°„è®°å½•ã€‚")
        return pd.DataFrame(columns=["uniprot_id", *df_gene_go.columns])

    mapping_df = pd.concat(all_batches, ignore_index=True).drop_duplicates()

    # ç»Ÿä¸€ç±»å‹ï¼Œé¿å…åˆå¹¶æ—¶ç±»å‹ä¸åŒ¹é…
    df_gene_go["gene_id"] = df_gene_go["gene_id"].astype(str)
    mapping_df["gene_id"] = mapping_df["gene_id"].astype(str)

    # ğŸ”— åˆå¹¶åˆ°åŸå§‹ geneâ€“GO è¡¨ï¼ˆä¿ç•™æ‰€æœ‰åˆ—ï¼‰
    merged = pd.merge(df_gene_go, mapping_df, on="gene_id", how="inner")

    # ä¿ç•™ uniprot_id + åŸå§‹æ‰€æœ‰åˆ—ï¼ˆgene_id åŒ…å«åœ¨å†…ï¼‰
    df_protein_go = merged[["uniprot_id", *df_gene_go.columns]]

    print(f"âœ… æˆåŠŸæ˜ å°„ {len(df_protein_go)} æ¡è®°å½•ï¼Œæ¥è‡ª {len(gene_ids)} ä¸ªåŸºå› ã€‚")
    return df_protein_go


def get_protein_domains(dbpath, protein_list, batch_size=500):
    """
    ä» uniprot_sprot_ft å’Œ uniprot_trembl_ft ä¸­æ‰¹é‡è·å–è›‹ç™½çš„ DOMAIN æ³¨é‡Šã€‚

    å‚æ•°:
        dbpath (str): æ•°æ®åº“è·¯å¾„ï¼ˆåŒ…å« database.infoï¼‰
        protein_list (list[str]): è¦æŸ¥è¯¢çš„ Uniprot ID åˆ—è¡¨
        batch_size (int): æ¯æ‰¹æŸ¥è¯¢æ•°é‡ï¼Œé»˜è®¤ 500ï¼Œé˜²æ­¢ SQL å¤ªé•¿

    è¿”å›:
        pd.DataFrame: åŒ…å« (uniprot_id, feature_type, start_pos, end_pos, note, evidence)
                      ä»…ä¿ç•™ feature_type='DOMAIN' çš„è¡Œ
    """
    all_results = []

    # å»é‡
    protein_list = list(set(protein_list))
    print(f"ğŸ” å…±éœ€æŸ¥è¯¢ {len(protein_list)} ä¸ª Uniprot ID")

    for i in tqdm(range(0, len(protein_list), batch_size)):
        batch = protein_list[i : i + batch_size]
        id_list = "', '".join(batch)

        sql = f"""
        COPY (
            SELECT accession AS uniprot_id, feature_type, start_pos, end_pos, note, evidence
            FROM uniprot_sprot_ft
            WHERE feature_type = 'DOMAIN' AND accession IN ('{id_list}')
            UNION ALL
            SELECT accession AS uniprot_id, feature_type, start_pos, end_pos, note, evidence
            FROM uniprot_trembl_ft
            WHERE feature_type = 'DOMAIN' AND accession IN ('{id_list}')
        ) TO STDOUT WITH (FORMAT CSV, DELIMITER E'\\t', HEADER);
        """

        try:
            output = pg_exec(dbpath, sql=sql)
            if output.strip():
                df = pd.read_csv(StringIO(output), sep="\t")
                all_results.append(df)
        except Exception as e:
            print(f"âš ï¸ æ‰¹æ¬¡ {i // batch_size + 1} æŸ¥è¯¢å¤±è´¥: {e}")

    if all_results:
        df_all = pd.concat(all_results, ignore_index=True)
        print(f"âœ… å…±è·å– {len(df_all)} æ¡ domain è®°å½•ã€‚")
        return df_all
    else:
        print("âš ï¸ æœªæŸ¥è¯¢åˆ°ä»»ä½• DOMAIN æ³¨é‡Šã€‚")
        return pd.DataFrame(
            columns=[
                "uniprot_id",
                "feature_type",
                "start_pos",
                "end_pos",
                "note",
                "evidence",
            ]
        )


def get_protein_go_terms(dbpath, protein_list, batch_size=500):
    """
    ä» uniprot_sprot_dr å’Œ uniprot_trembl_dr ä¸­æ‰¹é‡è·å–è›‹ç™½å¯¹åº”çš„ GO æ³¨é‡Šã€‚

    å‚æ•°:
        dbpath (str): æ•°æ®åº“è·¯å¾„ï¼ˆåŒ…å« database.infoï¼‰
        protein_list (list[str]): è¦æŸ¥è¯¢çš„ Uniprot accession åˆ—è¡¨
        batch_size (int): æ¯æ‰¹æŸ¥è¯¢æ•°é‡ï¼Œé»˜è®¤ 500

    è¿”å›:
        pd.DataFrame: åŒ…å«ä¸¤åˆ— ["uniprot_id", "go_id"]
    """
    all_results = []
    protein_list = list(set(protein_list))
    print(f"ğŸ” å…±éœ€æŸ¥è¯¢ {len(protein_list)} ä¸ª Uniprot ID")

    for i in tqdm(range(0, len(protein_list), batch_size)):
        batch = protein_list[i : i + batch_size]
        id_list = "', '".join(batch)

        sql = f"""
        COPY (
            SELECT accession AS uniprot_id, db_id AS go_id
            FROM uniprot_sprot_dr
            WHERE db_name = 'GO' AND accession IN ('{id_list}')
            UNION ALL
            SELECT accession AS uniprot_id, db_id AS go_id
            FROM uniprot_trembl_dr
            WHERE db_name = 'GO' AND accession IN ('{id_list}')
        ) TO STDOUT WITH (FORMAT CSV, DELIMITER E'\\t', HEADER);
        """

        try:
            output = pg_exec(dbpath, sql=sql)
            if output.strip():
                df = pd.read_csv(StringIO(output), sep="\t")
                all_results.append(df)
        except Exception as e:
            print(f"âš ï¸ æ‰¹æ¬¡ {i // batch_size + 1} æŸ¥è¯¢å¤±è´¥: {e}")

    if all_results:
        df_all = pd.concat(all_results, ignore_index=True)
        print(f"âœ… å…±è·å– {len(df_all)} æ¡ GO æ³¨é‡Šè®°å½•ã€‚")
        return df_all
    else:
        print("âš ï¸ æœªæŸ¥è¯¢åˆ°ä»»ä½• GO æ³¨é‡Šã€‚")
        return pd.DataFrame(columns=["uniprot_id", "go_id"])

def get_protein_interpro_terms(
    dbpath,
    protein_list,
    filter_types=None,
    remove_child_relations=False,
):
    """
    ä» uniprot_sprot_dr å’Œ uniprot_trembl_dr ä¸­æ‰¹é‡è·å–è›‹ç™½å¯¹åº”çš„ InterPro æ³¨é‡Šã€‚
    ï¼ˆæ—  for å¾ªç¯ç‰ˆï¼Œä½¿ç”¨ COPY TO æ–‡ä»¶å¯¼å‡ºå…¨é‡ç»“æœï¼‰

    å‚æ•°:
        dbpath (str): æ•°æ®åº“è·¯å¾„ï¼ˆåŒ…å« database.infoï¼‰
        protein_list (list[str]): è¦æŸ¥è¯¢çš„ Uniprot accession åˆ—è¡¨
        filter_types (list[str] | None): é™å®š interpro_entry.typeï¼Œä¾‹å¦‚ ["Domain", "Binding_site"]
        remove_child_relations (bool): æ˜¯å¦æ¶ˆé™¤çˆ¶å­å…³ç³»ï¼ˆä¿ç•™çˆ¶èŠ‚ç‚¹ï¼‰

    è¿”å›:
        pd.DataFrame: åŒ…å« ["uniprot_id", "interpro_id"]
    """

    protein_list = list(set(protein_list))
    print(f"ğŸ” å…±éœ€æŸ¥è¯¢ {len(protein_list)} ä¸ª Uniprot ID")

    if len(protein_list) == 0:
        return pd.DataFrame(columns=["uniprot_id", "interpro_id"])

    # === Step 1: åˆ›å»ºä¸´æ—¶è¡¨å¹¶å¯¼å…¥è›‹ç™½ID ===
    tmp_table_name = "tmp_protein_ids"
    tmp_file_in = tempfile.mktemp(suffix=".csv")
    pd.Series(protein_list, name="uniprot_id").to_csv(tmp_file_in, index=False, header=False)

    sql_create = f"""
    DROP TABLE IF EXISTS {tmp_table_name};
    CREATE TABLE {tmp_table_name} (uniprot_id TEXT);
    """
    pg_exec(dbpath=dbpath, sql=sql_create)

    sql_copy_in = f"\\COPY {tmp_table_name} FROM '{tmp_file_in}' WITH (FORMAT csv);"
    pg_exec(dbpath=dbpath, sql=sql_copy_in)
    os.remove(tmp_file_in)
    print(f"âœ… {len(protein_list)} æ¡è›‹ç™½ accession å·²å¯¼å…¥ {tmp_table_name}ã€‚")

    # === Step 2: å¦‚æœé™å®šç±»å‹ï¼Œå…ˆå–å‡ºå…è®¸çš„ InterPro ID é›†åˆ ===
    interpro_filter_set = None
    if filter_types:
        type_list = "', '".join(filter_types)
        filter_sql = f"""
        COPY (
            SELECT ipr_id AS interpro_id
            FROM interpro_entry
            WHERE type IN ('{type_list}')
        ) TO STDOUT WITH (FORMAT CSV, DELIMITER E'\\t', HEADER);
        """
        df_type = pd.read_csv(StringIO(pg_exec(dbpath=dbpath, sql=filter_sql)), sep="\t")
        interpro_filter_set = set(df_type["interpro_id"].tolist())
        print(f"âœ… é™å®š InterPro ç±»å‹ä¸º {filter_types}ï¼Œä¿ç•™ {len(interpro_filter_set)} æ¡è®°å½•ã€‚")

    # === Step 3: ä¸€æ¬¡æ€§ JOIN å¯¼å‡ºæ‰€æœ‰åŒ¹é…ç»“æœ ===
    tmp_file_out = tempfile.mktemp(suffix=".csv")
    sql_export = f"""
    COPY (
        SELECT s.accession AS uniprot_id, s.db_id AS interpro_id
        FROM uniprot_sprot_dr s
        WHERE s.db_name = 'InterPro'
        AND s.accession IN (SELECT uniprot_id FROM {tmp_table_name})
        UNION ALL
        SELECT t1.accession AS uniprot_id, t1.db_id AS interpro_id
        FROM uniprot_trembl_dr t1
        WHERE t1.db_name = 'InterPro'
        AND t1.accession IN (SELECT uniprot_id FROM {tmp_table_name})
    ) TO '{tmp_file_out}' WITH (FORMAT csv, DELIMITER E'\\t', HEADER);
    """
    pg_exec(dbpath=dbpath, sql=sql_export)
    pg_exec(dbpath=dbpath, sql=f"DROP TABLE IF EXISTS {tmp_table_name};")
    print(f"ğŸ“‚ å·²ç”ŸæˆåŒ¹é…ç»“æœæ–‡ä»¶: {tmp_file_out}")

    # === Step 4: è¯»å–ç»“æœå¹¶è¿‡æ»¤ ===
    df_all = pd.read_csv(tmp_file_out, sep="\t").drop_duplicates()
    os.remove(tmp_file_out)
    print(f"âœ… å…±è¯»å– {len(df_all)} æ¡ InterPro æ³¨é‡Šã€‚")

    if interpro_filter_set is not None:
        before = len(df_all)
        df_all = df_all[df_all["interpro_id"].isin(interpro_filter_set)]
        print(f"ğŸ§© å·²æ ¹æ®ç±»å‹ç­›é€‰ï¼Œä¿ç•™ {len(df_all)} æ¡ï¼ˆè¿‡æ»¤æ‰ {before - len(df_all)} æ¡ï¼‰ã€‚")

    # === Step 5: å»é™¤çˆ¶å­å…³ç³»çš„å­èŠ‚ç‚¹ï¼ˆå¯é€‰ï¼‰ ===
    if remove_child_relations:
        print("ğŸ§¬ æ­£åœ¨åŠ è½½ InterPro çˆ¶å­å…³ç³»è¡¨ä»¥ç§»é™¤å­é¡¹...")
        rel_sql = """
        COPY (
            SELECT parent, child
            FROM interpro_relation
        ) TO STDOUT WITH (FORMAT CSV, DELIMITER E'\\t', HEADER);
        """
        df_rel = pd.read_csv(StringIO(pg_exec(dbpath=dbpath, sql=rel_sql)), sep="\t")
        child_set = set(df_rel["child"])
        before = len(df_all)
        df_all = df_all[~df_all["interpro_id"].isin(child_set)]
        after = len(df_all)
        print(f"âœ… å·²ç§»é™¤ {before - after} æ¡å­å±‚çº§æ³¨é‡Šï¼Œä»…ä¿ç•™çˆ¶å±‚çº§ã€‚")

    print(f"ğŸ¯ æœ€ç»ˆè¾“å‡º {len(df_all)} æ¡ InterPro æ³¨é‡Šè®°å½•ã€‚")
    return df_all

def convert_gene_to_best_protein_with_interpro(
    dbpath,
    df_gene_go,
    filter_types=None,
    remove_child_relations=False,
):
    """
    å°† (gene_id, go_id, weight, ...) æ˜ å°„ä¸ºæœ€ä¼˜ uniprot_id
    ä½¿ç”¨ PostgreSQL COPY TO æ–‡ä»¶æ–¹å¼ï¼ˆé«˜é€Ÿã€ä½å†…å­˜ï¼‰
    æ”¹ä¸ºæ°¸ä¹…è¡¨æ¨¡å¼ï¼Œè‡ªåŠ¨æ£€æŸ¥/åˆ é™¤æ—§è¡¨ã€‚

    æ­¥éª¤:
      1. å°† gene_id å†™å…¥æŒä¹…è¡¨ tmp_gene_ids
      2. JOIN uniprot_idmapping è¡¨è·å¾— geneâ†’uniprot æ˜ å°„
      3. å¯¼å‡ºåˆ°ä¸´æ—¶ CSV æ–‡ä»¶å¹¶è¯»å…¥ pandas
      4. è®¡ç®— InterPro æ³¨é‡Šæ•°é‡å¹¶æŒ‘é€‰æœ€ä¼˜è›‹ç™½
      5. åˆå¹¶åŸ geneâ€“GO è¡¨ï¼Œè¿”å› df_protein_go
    """

    start_time = time.time()
    gene_ids = df_gene_go["gene_id"].astype(str).unique().tolist()
    print(f"ğŸ” æ£€æµ‹åˆ° {len(gene_ids)} ä¸ªå”¯ä¸€ GeneIDï¼Œå‡†å¤‡åˆ›å»ºæŒä¹…è¡¨...")

    # === Step 1. å°† GeneID å†™å…¥æŒä¹…è¡¨ ===
    tmp_table_name = "tmp_gene_ids"

    sql_check_drop = f"""
    DROP TABLE IF EXISTS {tmp_table_name};
    CREATE TABLE {tmp_table_name} (gene_id TEXT);
    """
    pg_exec(dbpath=dbpath, sql=sql_check_drop)

    tmp_gene_path = tempfile.mktemp(suffix=".csv")
    pd.Series(gene_ids, name="gene_id").to_csv(tmp_gene_path, index=False, header=False)

    sql_copy_in = f"\\COPY {tmp_table_name} FROM '{tmp_gene_path}' WITH (FORMAT csv);"
    pg_exec(dbpath=dbpath, sql=sql_copy_in)
    os.remove(tmp_gene_path)
    print(f"âœ… GeneID å·²å¯¼å…¥è¡¨ {tmp_table_name}ã€‚")

    # === Step 2. COPY JOIN ç»“æœç›´æ¥å†™åˆ°æ–‡ä»¶ ===
    tmp_out_path = tempfile.mktemp(suffix=".csv")
    sql_export = f"""
    COPY (
        SELECT t.gene_id, m.uniprot_id
        FROM {tmp_table_name} t
        JOIN uniprot_idmapping m
        ON t.gene_id = m.db_id
        WHERE m.db_name = 'GeneID'
    ) TO '{tmp_out_path}' WITH (FORMAT csv, DELIMITER E'\\t', HEADER);
    """
    pg_exec(dbpath=dbpath, sql=sql_export)
    print(f"ğŸ“‚ å·²ç”Ÿæˆæ˜ å°„ç»“æœæ–‡ä»¶: {tmp_out_path}")

    # === Step 3. è¯»å…¥ç»“æœ ===
    df_map = pd.read_csv(tmp_out_path, sep="\t").drop_duplicates()
    os.remove(tmp_out_path)
    print(f"âœ… è·å– {len(df_map)} æ¡ Geneâ€“Uniprot æ˜ å°„è®°å½•ã€‚")

    # === Step 4. æ¸…ç†æŒä¹…è¡¨ ===
    pg_exec(dbpath=dbpath, sql=f"DROP TABLE IF EXISTS {tmp_table_name};")
    print(f"ğŸ§¹ å·²æ¸…ç†è¡¨ {tmp_table_name}ã€‚")

    if df_map.empty:
        print("âš ï¸ æœªæ‰¾åˆ°ä»»ä½• GeneIDâ€“Uniprot æ˜ å°„ã€‚")
        return pd.DataFrame(columns=["uniprot_id", *df_gene_go.columns])

    # === Step 5. è·å– InterPro æ³¨é‡Šä¿¡æ¯ ===
    all_proteins = df_map["uniprot_id"].unique().tolist()
    print(f"ğŸ”¬ æ£€æµ‹åˆ° {len(all_proteins)} ä¸ªå”¯ä¸€è›‹ç™½ï¼Œå¼€å§‹è·å– InterPro æ³¨é‡Š...")

    df_ipr = get_protein_interpro_terms(
        dbpath=dbpath,
        protein_list=all_proteins,
        filter_types=filter_types,
        remove_child_relations=remove_child_relations,
    )

    if df_ipr.empty:
        print("âš ï¸ æœªè·å–åˆ° InterPro æ³¨é‡Šï¼Œé»˜è®¤ä¿ç•™é¦–ä¸ªæ˜ å°„ã€‚")
        best_map = df_map.groupby("gene_id").first().reset_index()
    else:
        ipr_count = (
            df_ipr.groupby("uniprot_id")["interpro_id"]
            .nunique()
            .reset_index(name="ipr_count")
        )
        df_map = df_map.merge(ipr_count, on="uniprot_id", how="left").fillna({"ipr_count": 0})

        # === Step 6. é€‰æ‹©æœ€ä¼˜è›‹ç™½ï¼ˆsprot ä¼˜å…ˆï¼‰ ===
        print("âš™ï¸ æ­£åœ¨é€‰æ‹©æœ€ä¼˜è›‹ç™½...")
        def select_best(group):
            sprot = group[group["uniprot_id"].str.startswith("P")]
            trembl = group[~group["uniprot_id"].str.startswith("P")]
            if len(sprot) >= 1:
                return sprot.loc[sprot["ipr_count"].idxmax()]
            elif len(trembl) > 0:
                return trembl.loc[trembl["ipr_count"].idxmax()]
            else:
                return group.iloc[0]

        best_map = (
            df_map.groupby("gene_id", group_keys=False)
            .apply(select_best)
            .reset_index(drop=True)
        )
        print(f"âœ… å·²ä¸º {len(best_map)} ä¸ªåŸºå› é€‰æ‹©æœ€ä¼˜è›‹ç™½ã€‚")

    # === Step 7. åˆå¹¶åŸå§‹ geneâ€“GO è¡¨ ===
    df_gene_go["gene_id"] = df_gene_go["gene_id"].astype(str)
    best_map["gene_id"] = best_map["gene_id"].astype(str)
    merged = pd.merge(df_gene_go, best_map[["gene_id", "uniprot_id"]], on="gene_id", how="inner")
    df_protein_go = merged[["uniprot_id", *df_gene_go.columns]]

    total_genes = df_gene_go["gene_id"].nunique()
    mapped_genes = best_map["gene_id"].nunique()
    unmapped = total_genes - mapped_genes

    print(f"ğŸ¯ æˆåŠŸç”Ÿæˆ {len(df_protein_go)} æ¡æ˜ å°„è®°å½•ï¼Œè¦†ç›– {mapped_genes}/{total_genes} ä¸ªåŸºå› ã€‚")
    if unmapped > 0:
        print(f"âš ï¸ å…¶ä¸­ {unmapped} ä¸ªåŸºå› æœªèƒ½æ‰¾åˆ°å¯¹åº”çš„ Uniprot IDã€‚")
    print(f"â±ï¸ æ€»è€—æ—¶ {time.time() - start_time:.2f} ç§’ã€‚")

    return df_protein_go